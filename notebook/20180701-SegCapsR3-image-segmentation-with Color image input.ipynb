{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegCapsR3 on Image Segmentation for Person\n",
    "## Input Color image files map to 24 bits data space.\n",
    "## loss function = dice\n",
    "## 10 epochs, 1,000 iterations per epoch.\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.001_recon-5.0_model_20180703-152449.hdf5\") # poor\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180703-180852.hdf5\") # good\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180703-210306.hdf5\") # ?\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-20.0_model_20180703-221150.hdf5\") # ?\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.1_recon-20.0_model_20180703-225112.hdf5\") # better\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.0001_recon-20.0_model_20180703-234853.hdf5\") # best\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.001_recon-0.5_model_20180704-030457.hdf5\") # ok\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-0_loss-dice_slic-1_sub--1_strid-1_lr-0.0001_recon-20.0_model_20180705-092846.hdf5\") # best\n",
    "\n",
    "\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 16) 416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 256, 256, 2,  12832       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_1 (ConvCapsuleLayer) (None, 256, 256, 4,  25664       primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_2 (ConvCapsuleLayer) (None, 128, 128, 4,  51328       conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_1 (ConvCapsuleLayer) (None, 128, 128, 8,  205056      conv_cap_2_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_2 (ConvCapsuleLayer) (None, 64, 64, 8, 64 410112      conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_4_1 (ConvCapsuleLayer) (None, 64, 64, 8, 32 409856      conv_cap_3_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_1 (DeconvCapsuleLa (None, 128, 128, 8,  131328      conv_cap_4_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_1 (Concatenate)              (None, 128, 128, 16, 0           deconv_cap_1_1[0][0]             \n",
      "                                                                 conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_2 (ConvCapsuleLaye (None, 128, 128, 4,  102528      up_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_1 (DeconvCapsuleLa (None, 256, 256, 4,  32832       deconv_cap_1_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_2 (Concatenate)              (None, 256, 256, 8,  0           deconv_cap_2_1[0][0]             \n",
      "                                                                 conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_2 (ConvCapsuleLaye (None, 256, 256, 4,  25664       up_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_3_1 (DeconvCapsuleLa (None, 512, 512, 2,  8224        deconv_cap_2_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_3 (Concatenate)              (None, 512, 512, 3,  0           deconv_cap_3_1[0][0]             \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  272         up_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 512, 512, 16) 0           mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,425,649\n",
      "Trainable params: 1,425,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-06 14:58:18.950678\n",
      "1/1 [==============================] - 39s 39s/step\n",
      "2018-07-06 14:58:58.255580\n"
     ]
    }
   ],
   "source": [
    "img = ['train8.png']\n",
    "# img = ['train-i-0.png']\n",
    "# img = ['experiment/train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_image(img_data,\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "# output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "#                                                                   net_input_shape,\n",
    "#                                                                   batchSize=1,\n",
    "#                                                                   numSlices=1,\n",
    "#                                                                   subSampAmt=0,\n",
    "#                                                                   stride=1),\n",
    "#                                             steps=1, max_queue_size=1, workers=1,\n",
    "#                                             use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.42331934 0.45820096 0.45726624 ... 0.46800333 0.46237186 0.40773   ]\n",
      "  [0.42583472 0.493165   0.47702125 ... 0.47784147 0.48269746 0.45138806]\n",
      "  [0.47968814 0.50487715 0.50502783 ... 0.5089212  0.5054481  0.47044823]\n",
      "  ...\n",
      "  [0.45158112 0.5134565  0.5004925  ... 0.50127774 0.51722014 0.47037914]\n",
      "  [0.46626952 0.49708995 0.50534695 ... 0.48278904 0.4947019  0.4575199 ]\n",
      "  [0.40934816 0.48467568 0.4614784  ... 0.47418663 0.4609347  0.43463123]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.5419511034269817\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de0023b710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1de62757a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "# plt.imsave('segcapsr3-lr-0.001-iter1000-ep7' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
