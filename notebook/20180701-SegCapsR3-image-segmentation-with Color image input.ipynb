{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegCapsR3 on Image Segmentation for Person\n",
    "## Input Color image files map to 24 bits data space.\n",
    "## loss function = dice\n",
    "## 10 epochs, 1,000 iterations per epoch.\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-5.0_model_20180703-030907.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 16) 416         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 256, 256, 2,  12832       reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_1 (ConvCapsuleLayer) (None, 256, 256, 4,  25664       primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_2 (ConvCapsuleLayer) (None, 128, 128, 4,  51328       conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_1 (ConvCapsuleLayer) (None, 128, 128, 8,  205056      conv_cap_2_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_2 (ConvCapsuleLayer) (None, 64, 64, 8, 64 410112      conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_4_1 (ConvCapsuleLayer) (None, 64, 64, 8, 32 409856      conv_cap_3_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_1 (DeconvCapsuleLa (None, 128, 128, 8,  131328      conv_cap_4_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_1 (Concatenate)              (None, 128, 128, 16, 0           deconv_cap_1_1[0][0]             \n",
      "                                                                 conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_2 (ConvCapsuleLaye (None, 128, 128, 4,  102528      up_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_1 (DeconvCapsuleLa (None, 256, 256, 4,  32832       deconv_cap_1_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_2 (Concatenate)              (None, 256, 256, 8,  0           deconv_cap_2_1[0][0]             \n",
      "                                                                 conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_2 (ConvCapsuleLaye (None, 256, 256, 4,  25664       up_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_3_1 (DeconvCapsuleLa (None, 512, 512, 2,  8224        deconv_cap_2_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_3 (Concatenate)              (None, 512, 512, 3,  0           deconv_cap_3_1[0][0]             \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  272         up_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mask_5 (Mask)                   (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 512, 512, 16) 0           mask_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,425,649\n",
      "Trainable params: 1,425,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-03 00:34:24.222807\n",
      "1/1 [==============================] - 42s 42s/step\n",
      "2018-07-03 00:35:06.452234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = ['1.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_image(img_data,\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "# output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "#                                                                   net_input_shape,\n",
    "#                                                                   batchSize=1,\n",
    "#                                                                   numSlices=1,\n",
    "#                                                                   subSampAmt=0,\n",
    "#                                                                   stride=1),\n",
    "#                                             steps=1, max_queue_size=1, workers=1,\n",
    "#                                             use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.44373026 0.58146715 0.5949665  ... 0.5403985  0.5456882  0.44797516]\n",
      "  [0.4643719  0.51443565 0.5233192  ... 0.46688125 0.49440968 0.44459873]\n",
      "  [0.5046051  0.553662   0.5512267  ... 0.47387484 0.5627133  0.4688009 ]\n",
      "  ...\n",
      "  [0.5346428  0.6304025  0.6276171  ... 0.6467196  0.63026834 0.47615603]\n",
      "  [0.5550109  0.6007725  0.6180238  ... 0.6189994  0.61472225 0.52455467]\n",
      "  [0.42339644 0.51764184 0.54790807 ... 0.52884114 0.56351286 0.41569123]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.5801365224760957\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18008898898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(output[0,:,:], cmap='gray')\n",
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "plt.imsave('segcapsr3-lr-0.001-iter1000-ep7' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mask = output2d[...] > threshold_level\n",
    "\n",
    "\n",
    "# Set all masked pixels to zero\n",
    "# masked = img_data.copy()\n",
    "# masked[mask] = 0\n",
    "# # print(masked)\n",
    "# # output_mask = sitk.GetImageFromArray(output_bin[0,:,:], isVector=True)\n",
    "# #     output_img = np.reshape(output_img, [512, 512, 1])\n",
    "# #     output_mask = np.reshape(output_mask, [512, 512, 1])\n",
    "\n",
    "# Display original and masked images side-by-side\n",
    "# print('threshold_level=%f'%threshold_level)\n",
    "# plt.figure()\n",
    "# f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "# ax0.imshow(img_data)\n",
    "# ax1.imshow(masked)\n",
    "# plt.axis('off')\n",
    "\n",
    "\n",
    "# cv2.imshow(\"Masked\",masked)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-88d9c25850e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGreys_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# plt.imsave('./output_img.png', output2d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output2d' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(output2d, cmap=plt.cm.Greys_r)\n",
    "plt.axis('off');\n",
    "# plt.imsave('./output_img.png', output2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate with Video Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import cv2\n",
    "import colorsys\n",
    "from IPython import display\n",
    "import signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definition for FPS calculation and webcam threading.\n",
    "\n",
    "#### Reference\n",
    "1. Adrian Rosebrock, imutils, https://github.com/jrosebr1/imutils/tree/master/imutils/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPS:\n",
    "    def __init__(self):\n",
    "        # store the start time, end time, and total number of frames\n",
    "        # that were examined between the start and end intervals\n",
    "        self._start = None\n",
    "        self._end = None\n",
    "        self._numFrames = 0\n",
    "\n",
    "    def start(self):\n",
    "        # start the timer\n",
    "        self._start = datetime.now()\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        # stop the timer\n",
    "        self._end = datetime.now()\n",
    "\n",
    "    def update(self):\n",
    "        # increment the total number of frames examined during the\n",
    "        # start and end intervals\n",
    "        self._numFrames += 1\n",
    "\n",
    "    def elapsed(self):\n",
    "        # return the total number of seconds between the start and\n",
    "        # end interval\n",
    "        return (self._end - self._start).total_seconds()\n",
    "\n",
    "    def fps(self):\n",
    "        # compute the (approximate) frames per second\n",
    "        return self._numFrames / self.elapsed()\n",
    "\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src=0, name=\"WebcamVideoStream\"):\n",
    "        # initialize the video camera stream and read the first frame\n",
    "        # from the stream\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "        # initialize the thread name\n",
    "        self.name = name\n",
    "        \n",
    "        # initialize the variable used to indicate if the thread should\n",
    "        # be stopped\n",
    "        self.stopped = False\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        # start the thread to read frames from the video stream\n",
    "        t = Thread(target=self.update, name=self.name, args=())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # keep looping infinitely until the thread is stopped\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped:\n",
    "                return\n",
    "            # otherwise, read the next frame from the stream\n",
    "            (self.grabbed, self.frame) = self.stream.read()           \n",
    "            \n",
    "    def read(self):\n",
    "        # return the frame most recently read\n",
    "        return self.frame\n",
    "\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Major display function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colors(N):\n",
    "    np.random.seed(70)\n",
    "    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
    "    return colors\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.4):\n",
    "    \"\"\"apply mask to image\"\"\"\n",
    "    for n, c in enumerate(color):\n",
    "        image[:, :, n] = np.where(\n",
    "            mask == 1,\n",
    "            image[:, :, n] * (1 - alpha) + alpha * c,\n",
    "            image[:, :, n]\n",
    "        )\n",
    "    return image\n",
    "\n",
    "\n",
    "def display_instances(image, boxes, masks, ids, names, scores):\n",
    "    \"\"\"\n",
    "        take the image and results and apply the mask, box, and Label\n",
    "    \"\"\"\n",
    "    n_instances = boxes.shape[0]\n",
    "    colors = random_colors(n_instances)\n",
    "\n",
    "    if not n_instances:\n",
    "        print('NO INSTANCES TO DISPLAY')\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n",
    "\n",
    "    for i, color in enumerate(colors):\n",
    "        if not np.any(boxes[i]):\n",
    "            continue\n",
    "\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        label = names[ids[i]]\n",
    "        score = scores[i] if scores is not None else None\n",
    "        caption = '{} {:.2f}'.format(label, score) if score else label\n",
    "        mask = masks[:, :, i]\n",
    "\n",
    "        image = apply_mask(image, mask, color)\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        image = cv2.putText(\n",
    "            image, caption, (x1+2, y1-7), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2\n",
    "        )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a signal function to capture the interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_handler(signal, frame):\n",
    "    # KeyboardInterrupt detected, exiting\n",
    "    global is_interrupted\n",
    "    is_interrupted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model (model_path, net_input_shape, num_class):\n",
    "    train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "\n",
    "    # Load weights trained on MS-COCO\n",
    "    eval_model.load_weights(model_path)\n",
    "\n",
    "    class_names = ['person']\n",
    "    return class_names, eval_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrupt the kernel to stop the capture or wait for 50 frames processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "1/1 [==============================] - 83s 83s/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-44f508449a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         frame = display_instances(\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rois'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'masks'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scores'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         )\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# check to see if the frame should be displayed to our screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "    num_class = 2    \n",
    "    class_names, model = get_model(COCO_MODEL_PATH, net_input_shape, num_class)\n",
    "\n",
    "    # created a *threaded* video stream, allow the camera sensor to warmup,\n",
    "    # and start the FPS counter\n",
    "    print(\"[INFO] sampling THREADED frames from webcam...\")\n",
    "    vs = WebcamVideoStream(src=0).start()\n",
    "    fps = FPS().start()\n",
    "    frame = vs.read()\n",
    "    \n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    is_interrupted = False\n",
    "\n",
    "    # loop over 50 frames...this time using the threaded stream\n",
    "    while fps._numFrames < 50:\n",
    "        # grab the frame from the threaded video stream\n",
    "        frame = vs.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # check to see if the frame should be displayed to our screen\n",
    "        results = model.predict_generator(generate_test_image(\n",
    "                                        test_img=frame, net_input_shape=net_input_shape,\n",
    "                                        batchSize=1, numSlices=1,\n",
    "                                        subSampAmt=0, stride=1, downSampAmt=1),\n",
    "                                        steps=1, max_queue_size=1, workers=1,\n",
    "                                        use_multiprocessing=False, verbose=1)\n",
    "        \n",
    "        \n",
    "        r = results[0]\n",
    "        frame = display_instances(\n",
    "                frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']\n",
    "        )\n",
    "        # check to see if the frame should be displayed to our screen\n",
    "        \n",
    "        plt.imshow(frame)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        try:    # Avoids a NotImplementedError caused by `plt.pause`\n",
    "            plt.pause(5.05) # the pause time\n",
    "        except Exception:\n",
    "            pass\n",
    "        # update the FPS counter\n",
    "        fps.update()\n",
    "        if is_interrupted:\n",
    "            break\n",
    "    # stop the timer and display FPS information\n",
    "    fps.stop()\n",
    "    print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "    print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    " \n",
    "    # do a bit of cleanup\n",
    "    vs.stop()\n",
    "#     cv2.destroyAllWindows()\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
